---
title: 'Workflow Overview'
description: 'n8n workflow architecture for async batch processing'
---

## Overview
LeadDesk 5 uses n8n to orchestrate asynchronous batch processing through a dispatcher and three specialized worker workflows.

## Workflow Components

| Workflow | Purpose |
|----------|---------|
| **Dispatcher** | Receives jobs and distributes practices to workers in batches |
| **URL Worker** | Normalizes and validates practice URLs |
| **Scraper Worker** | Extracts practice data using OpenAI |
| **Aggregator** | Combines data and produces final enriched payload |

## Processing Flow

1. Frontend calls POST `/api/jobs` with CSV file
2. Backend triggers Dispatcher webhook with `job_id` and `practice_count`
3. Dispatcher chunks practices into groups of 10-25 and triggers URL Worker
4. URL Worker normalizes each practice URL and validates domains
5. Scraper Worker fetches and extracts practice data using OpenAI
6. Aggregator Worker combines all payloads into final enriched data
7. Database triggers update job status to `completed`

## Status Transitions

Practice status flows through these states:

```mermaid
graph LR
    A[pending] --> B[url_complete]
    B --> C[scraped]
    C --> D[aggregated]
    A -.error.-> E[error]
    B -.error.-> E
    C -.error.-> E
```

| Status | Description |
|--------|-------------|
| `pending` | Practice awaiting URL worker |
| `url_complete` | URL normalized and validated |
| `scraped` | Practice data extracted |
| `aggregated` | Final enrichment complete |
| `error` | Processing failed at any stage |

## Error Handling

All workers implement error handling:

- **Retry Logic**: Failed practices can be retried
- **Error Messages**: Detailed error info stored in `error_message` field
- **Failed Jobs Table**: Critical failures logged to `failed_jobs`
- **Job Status**: Any practice error can trigger job-level `error` status

## Configuration

Workflow webhooks are configured via environment variables:

```bash
# Dispatcher
LD5_DISPATCHER_WEBHOOK_URL=https://n8n.example.com/webhook/dispatcher

# Workers (called by dispatcher)
N8N_URL_WORKER_WEBHOOK=https://n8n.example.com/webhook/url-worker
N8N_SCRAPER_WEBHOOK=https://n8n.example.com/webhook/scraper
N8N_AGGREGATOR_WEBHOOK=https://n8n.example.com/webhook/aggregator
```

## Performance Considerations

- **Batch Size**: 10-25 practices per batch (configurable)
- **Concurrency**: Workers process batches in parallel
- **Rate Limiting**: Implement delays between scraping requests
- **Resource Usage**: Monitor n8n instance CPU/memory

## Next Steps

- [Dispatcher Workflow](/docs/workflows/dispatcher) - Configure the dispatcher workflow
- [URL Worker](/docs/workflows/url-worker) - Set up URL normalization
- [Scraper Worker](/docs/workflows/scraper-worker) - Configure OpenAI scraping
- [Aggregator Worker](/docs/workflows/aggregator-worker) - Set up data aggregation
